{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing di segnali bi-dimensionali\n",
    "\n",
    "Il task è riconoscere l’identità di una persona a partire da una foto del\n",
    "volto\n",
    "- L’identità è da riconoscere all’interno del gruppo di lavoro\n",
    "- Il volto da riconoscere può essere ritagliato a mano (crop) o può essere utilizzato un qualunque face detecor (es. quello presente in OpenCV, NON è richiesto di implementarne uno!)\n",
    "\n",
    "\n",
    "Potete decidere voi il livello di difficoltà:\n",
    "- Volto frontale con espressione neutra e buone condizioni di illuminazione\n",
    "- Espressione variabile\n",
    "- Condizioni di illuminazione variabili (es. poca luce, luce direzionale, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv # pip install opencv-python\n",
    "import joblib\n",
    "\n",
    "from keras.applications import resnet50, xception, vgg16, inception_resnet_v2\n",
    "from keras.preprocessing import image as kimage\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.layers import Dense, GlobalMaxPooling2D ,GlobalAveragePooling2D, ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\n",
    "from keras.models import Model, load_model,  model_from_json\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import Sequential\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn import svm\n",
    "\n",
    "import joblib\n",
    "from skimage import feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@vinayakvarrier/building-a-real-time-face-recognition-system-using-pre-trained-facenet-model-f1a277a06947"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://flyyufelix.github.io/2016/10/03/fine-tuning-in-keras-part1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preparazione dataset training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obiettivo: Tramite face detector di OpenCV si estrapolano immagini di volti di Silvia e Giacomo da un set di immagini.\n",
    "Il risultato del face detector viene manualmente filtrato dalle immagini che non costituiscono volti di Silvia e Giacomo."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "maximages=500"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def face_detector(base_path='./image/raw/'):\n",
    "\n",
    "    face_detector = cv.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
    "    i=1\n",
    "    \n",
    "    for di,d in enumerate(sorted(os.listdir(base_path))):\n",
    "        for fi,f in enumerate(sorted(os.listdir(base_path + d + '/'))):\n",
    "\n",
    "            if  ( f.endswith('.jpg') or f.endswith('.jpeg') or f.endswith('.JPG')) and fi<maximages:\n",
    "                # Memorizza percorso file\n",
    "                cur_path = base_path + d + '/' + f\n",
    "\n",
    "                # Carica file ed estraine le features\n",
    "                img = cv.imread(cur_path)\n",
    "                img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "                faces = face_detector.detectMultiScale(img_gray)\n",
    "                \n",
    "                for x in range(0,len(faces)): \n",
    "                    (x,y,w,h) = faces[x]\n",
    "                    face = img[y:y+h,x:x+h,:]\n",
    "                    \n",
    "                    #resize\n",
    "                    (h, w) = face.shape[:2]\n",
    "                    r = w / float(w)\n",
    "                    dim = (w, int(h * r))\n",
    "                    resized = cv.resize(face, dim, interpolation = cv.INTER_AREA)\n",
    "\n",
    "                    cv.imwrite('./image/crop/'+ d + '/' + str(i) +'.jpg', resized)\n",
    "                    i+=1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "face_detector(base_path='./image/raw/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Trained VGG Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato il numero ridotto di immagini a disposizione (<100), si e' scelto come primo approcio per risolvere il problema in esame, l'utilizzo dei feature proveninneti da un pre-trained CNN, allenata su un adeguato numero di immagini per poter sostenere la stima di un cosi' ampio numero di parametri, necessari a questo modello.\n",
    "\n",
    "Le feature estratte verranno poi usate come input di un modello di classificazione.\n",
    "\n",
    "Inoltre, per la stessa ragione di scarsita' del campione, si e' scelto di non perseguire la strada del fine-tuning.\n",
    "\n",
    "Si e scelto di utilizzare il modello pretrained VGG Face, ovvero un modello VGG allenato su un set di immagini apposito per il riconoscimento di volti e molto simile alla tipologia di immagini che si intende classificare."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello e' composto da 22 layer, per estrarre le feature si e' deciso di utilizzare il risultato dell'ultimo layer prima dello strato fully-connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('vgg_face_weights.h5')\n",
    "vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_14_input (Inp (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_14 (ZeroPaddi (None, 226, 226, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPaddi (None, 226, 226, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPaddi (None, 114, 114, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPaddi (None, 114, 114, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_18 (ZeroPaddi (None, 58, 58, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_19 (ZeroPaddi (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_20 (ZeroPaddi (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_21 (ZeroPaddi (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_22 (ZeroPaddi (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_23 (ZeroPaddi (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_24 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_25 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_26 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 1, 1, 4096)        102764544 \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 1, 1, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 1, 1, 2622)        10742334  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2622)              0         \n",
      "=================================================================\n",
      "Total params: 145,002,878\n",
      "Trainable params: 145,002,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_face_descriptor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguit la funzione che verra' applicata ad ogni immagine di training per ottenere le feature neurali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_face_descriptor_features(img):\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    f = vgg_face_descriptor.predict(x)\n",
    "    return f.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caricamento Immagini con Estrazione Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximages=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature dummy\n",
    "def dummy(image):\n",
    "    return image\n",
    "\n",
    "\n",
    "# Data loader\n",
    "def load_data(feature_extractor=dummy, base_path='./image/filtered/', size=244):\n",
    "\n",
    "    paths = []\n",
    "    features = []\n",
    "    labels = []\n",
    "    labels_name = []\n",
    "    \n",
    "    for di,d in enumerate(sorted(os.listdir(base_path))):\n",
    "        for fi,f in enumerate(sorted(os.listdir(base_path + d + '/'))):\n",
    "            \n",
    "            if f.endswith('.jpg') and fi<maximages:\n",
    "                # Memorizza percorso file\n",
    "                cur_path = base_path + d + '/' + f\n",
    "                paths.append(cur_path)\n",
    "\n",
    "                # Carica file ed estraine le features\n",
    "                image = kimage.load_img(cur_path, target_size=(size, size)) #!!!!!!!!\n",
    "                cur_features = feature_extractor(image)\n",
    "                features.append(cur_features)\n",
    "                \n",
    "                # Categorie\n",
    "                labels.append(di)\n",
    "                labels_name.append(d)\n",
    "\n",
    "\n",
    "    features = np.array(features)\n",
    "    \n",
    "    # Separazione training / test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels_name, test_size=0.2, shuffle=True, random_state=1)\n",
    "    # X: features, y: labels\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento e feature extraction in 59.651 secondi\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "X_train, X_test, y_train, y_test = load_data(feature_extractor=vgg_face_descriptor_features, size=224)\n",
    "print(\"Caricamento e feature extraction in %0.3f secondi\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelli di classificazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDTree "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tree = KDTree(X_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "joblib.dump(tree, 'face_kdtree.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = None\n",
    "tree = joblib.load('face_kdtree.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, ind = tree.query(X_test, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [y_train[int(i)] for i in ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ind = tree.query(X_test, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [y_train[int(i)] for i in ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report di classificazione:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     giacomo       1.00      1.00      1.00         6\n",
      "      silvia       1.00      1.00      1.00        16\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        22\n",
      "   macro avg       1.00      1.00      1.00        22\n",
      "weighted avg       1.00      1.00      1.00        22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Report di classificazione:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello risulta prevedere perfettamente quando un immagine contiene silvia o giacomo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice di confusione:\n",
      "[[ 6  0]\n",
      " [ 0 16]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Matrice di confusione\n",
    "print(\"Matrice di confusione:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "plt.imshow(cm, cmap=plt.cm.Blues); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def process_frame(img):\n",
    "    return img\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "while(True):\n",
    "    # Cattura un nuovo frame\n",
    "    r, frame = cap.read()\n",
    "    # Elaborane il contenuto\n",
    "    frame = process_frame(frame)\n",
    "    # Mostralo in una finestra separata\n",
    "    cv.imshow('Video', frame)\n",
    "        \n",
    "    # Interrompi lo streaming premendo il tasto Q\n",
    "    if cv.waitKey(20) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "img = cv.imread('./thumbnails_features_deduped_publish/amber heard/2.jpg')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.imshow(frame), plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "faces = face_detector.detectMultiScale(img_gray)\n",
    "\n",
    "for (x,y,w,h) in faces:\n",
    "    if w > 60 : #avoid too small image\n",
    "        cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) #draw rectangle to main image\n",
    "\n",
    "        detected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
    "        print(detected_face.shape)\n",
    "        plt.imshow(detected_face), plt.show()\n",
    "        detected_face_res = cv.resize(detected_face, (224, 224)) #resize to 224x224\n",
    "\n",
    "\n",
    "        captured_features = vgg_face_descriptor_features(detected_face_res).reshape(1,-1)\n",
    "        dist, ind = tree.query(captured_features, k=1)\n",
    "        y_pred = [y_train[int(i)] for i in ind]\n",
    "\n",
    "        print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector = cv.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "cap = cv.VideoCapture(0) #webcam\n",
    "\n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(img_gray)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        if w > 130: \n",
    "            cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) #draw rectangle to main image\n",
    "\n",
    "            detected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
    "            detected_face = cv.resize(detected_face, (224, 224)) #resize to 224x224\n",
    "\n",
    "            captured_features = vgg_face_descriptor_features(detected_face).reshape(1,-1)\n",
    "            dist, ind = tree.query(captured_features, k=1)\n",
    "            y_pred = [y_train[int(i)] for i in ind][0]\n",
    "\n",
    "            color=(0,255,0)\n",
    "            \n",
    "            if (dist <= 90):\n",
    "                cv.putText(img, y_pred, (int(x+w+15), int(y-12)), cv.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "                #connect face and text\n",
    "            \n",
    "                cv.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),color,1)\n",
    "                cv.line(img,(x+w,y-20),(x+w+10,y-20),color,1)\n",
    "\n",
    "            else if (dist > 90): #if found image is too different\n",
    "                cv.putText(img, 'unknown', (int(x+w+15), int(y-12)), cv.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    cv.imshow('img',img)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'): #press q to quit\n",
    "        break\n",
    "\n",
    "#kill open cv things\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo modello funziona bene nella misura in cui deve distinguere tra le persone presenti, nel momento in cui si utilizzano le probabilita' predette per distinguere immagini di altri, la sua accuratezza non e' altrettanto precisa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf_svm = GridSearchCV(svm.SVC(kernel='rbf', class_weight='balanced', probability=True), param_grid, cv=5) #radial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 15.504s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\silvia.bordogna\\appdata\\local\\continuum\\anaconda2\\envs\\dsim\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "#Train the model using the training sets\n",
    "clf_svm.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf_svm.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['face_svm.sav']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf_svm, 'face_svm.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = None\n",
    "clf_svm = joblib.load('face_svm.sav') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the response for test dataset\n",
    "y_pred = clf_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01, 0.99],\n",
       "       [1.  , 0.  ],\n",
       "       [0.01, 0.99],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.01, 0.99],\n",
       "       [0.01, 0.99],\n",
       "       [0.02, 0.98],\n",
       "       [1.  , 0.  ],\n",
       "       [0.01, 0.99],\n",
       "       [0.01, 0.99],\n",
       "       [0.01, 0.99],\n",
       "       [0.01, 0.99],\n",
       "       [0.13, 0.87],\n",
       "       [1.  , 0.  ],\n",
       "       [0.53, 0.47],\n",
       "       [0.99, 0.01],\n",
       "       [0.01, 0.99],\n",
       "       [0.99, 0.01],\n",
       "       [0.09, 0.91],\n",
       "       [0.  , 1.  ],\n",
       "       [0.03, 0.97]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(clf_svm.predict_proba(X_test),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report di classificazione:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     giacomo       1.00      1.00      1.00         6\n",
      "      silvia       1.00      1.00      1.00        16\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        22\n",
      "   macro avg       1.00      1.00      1.00        22\n",
      "weighted avg       1.00      1.00      1.00        22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Report di classificazione:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo SVM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "img = cv.imread('./thumbnails_features_deduped_publish/adrien brody/11.jpg')\n",
    "plt.imshow(img), plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def process_frame(img):\n",
    "    return img\n",
    "cap = cv.VideoCapture(0)\n",
    "while(True):\n",
    "    # Cattura un nuovo frame\n",
    "    r, frame = cap.read()\n",
    "    # Elaborane il contenuto\n",
    "    frame = process_frame(frame)\n",
    "    # Mostralo in una finestra separata\n",
    "    cv.imshow('Video', frame)\n",
    "    # Interrompi lo streaming premento il tasto Q\n",
    "    if cv.waitKey(20) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "img = frame"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "faces = face_detector.detectMultiScale(img_gray)\n",
    "\n",
    "for (x,y,w,h) in faces:\n",
    "    if w > 60: \n",
    "        cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) #draw rectangle to main image\n",
    "\n",
    "        detected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
    "        detected_face = cv.resize(detected_face, (224, 224)) #resize to 224x224\n",
    "\n",
    "        captured_features = vgg_face_descriptor_features(detected_face).reshape(1,-1)\n",
    "        print((clf_svm.predict_proba(captured_features)), clf_svm.predict(captured_features))\n",
    "        prob = clf_svm.predict_proba(captured_features)\n",
    "        label=clf_svm.predict(captured_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector = cv.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "cap = cv.VideoCapture(0) #webcam\n",
    "\n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(img_gray)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        if w > 130: \n",
    "            cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) #draw rectangle to main image\n",
    "\n",
    "            detected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
    "            detected_face = cv.resize(detected_face, (224, 224)) #resize to 224x224\n",
    "\n",
    "            captured_features = vgg_face_descriptor_features(detected_face).reshape(1,-1)\n",
    "            pred_prob = clf_svm.predict_proba(captured_features)\n",
    "            pred_label = clf_svm.predict(captured_features)[0]\n",
    "\n",
    "            color=(0,255,0)\n",
    "            \n",
    "            if (np.max(pred_prob) >= 0.9):\n",
    "                cv.putText(img, pred_label, (int(x+w+15), int(y-12)), cv.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "                #connect face and text\n",
    "            \n",
    "                cv.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),color,1)\n",
    "                cv.line(img,(x+w,y-20),(x+w+10,y-20),color,1)\n",
    "\n",
    "            if (np.max(pred_prob)<0.9): #if found image is too different\n",
    "                cv.putText(img, 'unknown', (int(x+w+15), int(y-12)), cv.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    cv.imshow('img',img)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'): #press q to quit\n",
    "        break\n",
    "\n",
    "#kill open cv things\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un approccio alternativo calcolando misure di similarita' dirrettamente sulle feature neurali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCosineSimilarity(source_representation, test_representation):\n",
    "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
    "    b = np.sum(np.multiply(source_representation, source_representation))\n",
    "    c = np.sum(np.multiply(test_representation, test_representation))\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
    " \n",
    "def findEuclideanDistance(source_representation, test_representation):\n",
    "    euclidean_distance = source_representation - test_representation\n",
    "    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\n",
    "    euclidean_distance = np.sqrt(euclidean_distance)\n",
    "    return euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0) #webcam\n",
    "\n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    \n",
    "    img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(img_gray)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        if w > 130: \n",
    "            cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) #draw rectangle to main image\n",
    "\n",
    "            detected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
    "            detected_face = cv.resize(detected_face, (224, 224)) #resize to 224x224\n",
    "\n",
    "            captured_representation = vgg_face_descriptor_features(detected_face)\n",
    "\n",
    "            found = 0\n",
    "            for n, i in enumerate(X_train):\n",
    "                image_name = y_train[n]\n",
    "                representation = i\n",
    "\n",
    "                similarity = findCosineSimilarity(representation, captured_representation)\n",
    "                if(similarity < 0.30):\n",
    "                    cv.putText(img, image_name, (int(x+w+15), int(y-12)), cv.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "                    \n",
    "                    found = 1\n",
    "                    break\n",
    "\n",
    "            #connect face and text\n",
    "            color=(0,255,0)\n",
    "            cv.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),color,1)\n",
    "            cv.line(img,(x+w,y-20),(x+w+10,y-20),color,1)\n",
    "\n",
    "            if(found == 0): #if found image is not in employee database\n",
    "                cv.putText(img, 'unknown', (int(x+w+15), int(y-12)), cv.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    cv.imshow('img',img)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'): #press q to quit\n",
    "        break\n",
    "\n",
    "#kill open cv things\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
